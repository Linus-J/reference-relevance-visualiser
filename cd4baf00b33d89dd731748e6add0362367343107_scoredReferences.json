[
    [
        {
            "paperId": "acc8f7cc17ce5009bd2504572e8b9b76148e63a7",
            "title": "Adversarially Robust Conformal Prediction",
            "abstract": "Conformal prediction is a model-agnostic tool for constructing prediction sets that are valid under the common i.i.d. assumption, which has been applied to quantify the prediction uncertainty of deep net classifiers. In this paper, we generalize this framework to the case where adversaries exist during inference time, under which the i.i.d. assumption is grossly violated. By combining conformal prediction with randomized smoothing, our proposed method forms a prediction set with finite-sample coverage guarantee that holds for any data distribution with `2norm bounded adversarial noise, generated by any adversarial attack algorithm. The core idea is to bound the Lipschitz constant of the non-conformity score by smoothing it with Gaussian noise and leverage this knowledge to account for the effect of the unknown adversarial perturbation. We demonstrate the necessity of our method in the adversarial setting and the validity of our theoretical guarantee on three widely used benchmark data sets: CIFAR10, CIFAR100, and ImageNet.",
            "year": 2022,
            "authors": [
                {
                    "authorId": "2144258187",
                    "name": "Asaf Gendler"
                },
                {
                    "authorId": "27836724",
                    "name": "Tsui-Wei Weng"
                },
                {
                    "authorId": "144599989",
                    "name": "L. Daniel"
                },
                {
                    "authorId": "3295351",
                    "name": "Yaniv Romano"
                }
            ]
        },
        4.791629859439528,
        6,
        0.7986049765732546,
        "red"
    ],
    [
        {
            "paperId": "69cdf836d0e2ac8ef25ccdbcfb88374ce42c4707",
            "title": "Provably Robust Conformal Prediction with Improved Efficiency",
            "abstract": "Conformal prediction is a powerful tool to generate uncertainty sets with guaranteed coverage using any predictive model, under the assumption that the training and test data are i.i.d.. Recently, it has been shown that adversarial examples are able to manipulate conformal methods to construct prediction sets with invalid coverage rates, as the i.i.d. assumption is violated. To address this issue, a recent work, Randomized Smoothed Conformal Prediction (RSCP), was first proposed to certify the robustness of conformal prediction methods to adversarial noise. However, RSCP has two major limitations: (i) its robustness guarantee is flawed when used in practice and (ii) it tends to produce large uncertainty sets. To address these limitations, we first propose a novel framework called RSCP+ to provide provable robustness guarantee in evaluation, which fixes the issues in the original RSCP method. Next, we propose two novel methods, Post-Training Transformation (PTT) and Robust Conformal Training (RCT), to effectively reduce prediction set size with little computation overhead. Experimental results in CIFAR10, CIFAR100, and ImageNet suggest the baseline method only yields trivial predictions including full label set, while our methods could boost the efficiency by up to $4.36\\times$, $5.46\\times$, and $16.9\\times$ respectively and provide practical robustness guarantee. Our codes are available at https://github.com/Trustworthy-ML-Lab/Provably-Robust-Conformal-Prediction.",
            "year": 2024,
            "authors": [
                {
                    "authorId": "2298969339",
                    "name": "Ge Yan"
                },
                {
                    "authorId": "2298967659",
                    "name": "Yaniv Romano"
                },
                {
                    "authorId": "27836724",
                    "name": "Tsui-Wei Weng"
                }
            ]
        },
        4.165111773996238,
        5,
        0.8330223547992477,
        "red"
    ],
    [
        {
            "paperId": "199f67917bc2f4abf067df9301bc0bcb5657f203",
            "title": "Probabilistically robust conformal prediction",
            "abstract": "Conformal prediction (CP) is a framework to quantify uncertainty of machine learning classifiers including deep neural networks. Given a testing example and a trained classifier, CP produces a prediction set of candidate labels with a user-specified coverage (i.e., true class label is contained with high probability). Almost all the existing work on CP assumes clean testing data and there is not much known about the robustness of CP algorithms w.r.t natural/adversarial perturbations to testing examples. This paper studies the problem of probabilistically robust conformal prediction (PRCP) which ensures robustness to most perturbations around clean input examples. PRCP generalizes the standard CP (cannot handle perturbations) and adversarially robust CP (ensures robustness w.r.t worst-case perturbations) to achieve better trade-offs between nominal performance and robustness. We propose a novel adaptive PRCP (aPRCP) algorithm to achieve probabilistically robust coverage. The key idea behind aPRCP is to determine two parallel thresholds, one for data samples and another one for the perturbations on data (aka\"quantile-of-quantile\"design). We provide theoretical analysis to show that aPRCP algorithm achieves robust coverage. Our experiments on CIFAR-10, CIFAR-100, and ImageNet datasets using deep neural networks demonstrate that aPRCP achieves better trade-offs than state-of-the-art CP and adversarially robust CP algorithms.",
            "year": 2023,
            "authors": [
                {
                    "authorId": "2119606206",
                    "name": "Subhankar Ghosh"
                },
                {
                    "authorId": "5693856",
                    "name": "Yuanjie Shi"
                },
                {
                    "authorId": "1411834674",
                    "name": "Taha Belkhouja"
                },
                {
                    "authorId": "1987128531",
                    "name": "Yan Yan"
                },
                {
                    "authorId": "2089333",
                    "name": "J. Doppa"
                },
                {
                    "authorId": "2226235158",
                    "name": "Brian Jones"
                }
            ]
        },
        3.460479989534264,
        4,
        0.865119997383566,
        "blue"
    ],
    [
        {
            "paperId": "2a7d19594e07e9f5f40eb39f19b0e5ffa6aa5df9",
            "title": "Efficient Neural Network Robustness Certification with General Activation Functions",
            "abstract": "Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.",
            "year": 2018,
            "authors": [
                {
                    "authorId": "49723481",
                    "name": "Huan Zhang"
                },
                {
                    "authorId": "27836724",
                    "name": "Tsui-Wei Weng"
                },
                {
                    "authorId": "153191489",
                    "name": "Pin-Yu Chen"
                },
                {
                    "authorId": "1793529",
                    "name": "Cho-Jui Hsieh"
                },
                {
                    "authorId": "144599989",
                    "name": "L. Daniel"
                }
            ]
        },
        3.1832151503526456,
        6,
        0.5305358583921076,
        "red"
    ],
    [
        {
            "paperId": "f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed",
            "title": "Certified Adversarial Robustness via Randomized Smoothing",
            "abstract": "We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
            "year": 2019,
            "authors": [
                {
                    "authorId": "39951470",
                    "name": "Jeremy M. Cohen"
                },
                {
                    "authorId": "49686853",
                    "name": "Elan Rosenfeld"
                },
                {
                    "authorId": "145116464",
                    "name": "J. Z. Kolter"
                }
            ]
        },
        0.5269701889210994,
        2,
        0.5269701889210994,
        "blue"
    ],
    [
        {
            "paperId": "18a9bb863e3110e2e981b53618b214585a32f877",
            "title": "Automatic Perturbation Analysis for Scalable Certified Robustness and Beyond",
            "abstract": "Linear relaxation based perturbation analysis (LiRPA) for neural networks, which computes provable linear bounds of output neurons given a certain amount of input perturbation, has become a core component in robustness verification and certified defense. The majority of LiRPA-based methods focus on simple feed-forward networks and need particular manual derivations and implementations when extended to other architectures. In this paper, we develop an automatic framework to enable perturbation analysis on any neural network structures, by generalizing existing LiRPA algorithms such as CROWN to operate on general computational graphs. The flexibility, differentiability and ease of use of our framework allow us to obtain state-of-the-art results on LiRPA based certified defense on fairly complicated networks like DenseNet, ResNeXt and Transformer that are not supported by prior works. Our framework also enables loss fusion, a technique that significantly reduces the computational complexity of LiRPA for certified defense. For the first time, we demonstrate LiRPA based certified defense on Tiny ImageNet and Downscaled ImageNet where previous approaches cannot scale to due to the relatively large number of classes. Our work also yields an open-source library for the community to apply LiRPA to areas beyond certified defense without much LiRPA expertise, e.g., we create a neural network with a probably flat optimization landscape by applying LiRPA to network parameters. Our opensource library is available at https://github.com/KaidiXu/auto_LiRPA.",
            "year": 2020,
            "authors": [
                {
                    "authorId": "46321210",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2987927",
                    "name": "Zhouxing Shi"
                },
                {
                    "authorId": "2109067470",
                    "name": "Huan Zhang"
                },
                {
                    "authorId": "2108927851",
                    "name": "Yihan Wang"
                },
                {
                    "authorId": "2782886",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "1730108",
                    "name": "Minlie Huang"
                },
                {
                    "authorId": "1749353",
                    "name": "B. Kailkhura"
                },
                {
                    "authorId": "1662772707",
                    "name": "Xue Lin"
                },
                {
                    "authorId": "1793529",
                    "name": "Cho-Jui Hsieh"
                }
            ]
        },
        0.425641259192797,
        2,
        0.425641259192797,
        "red"
    ],
    [
        {
            "paperId": "01fe33d7147cfb08d4542402089535ed911b4024",
            "title": "Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers",
            "abstract": "Formal verification of neural networks (NNs) is a challenging and important problem. Existing efficient complete solvers typically require the branch-and-bound (BaB) process, which splits the problem domain into sub-domains and solves each sub-domain using faster but weaker incomplete verifiers, such as Linear Programming (LP) on linearly relaxed sub-domains. In this paper, we propose to use the backward mode linear relaxation based perturbation analysis (LiRPA) to replace LP during the BaB process, which can be efficiently implemented on the typical machine learning accelerators such as GPUs and TPUs. However, unlike LP, LiRPA when applied naively can produce much weaker bounds and even cannot check certain conflicts of sub-domains during splitting, making the entire procedure incomplete after BaB. To address these challenges, we apply a fast gradient based bound tightening procedure combined with batch splits and the design of minimal usage of LP bound procedure, enabling us to effectively use LiRPA on the accelerator hardware for the challenging complete NN verification problem and significantly outperform LP-based approaches. On a single GPU, we demonstrate an order of magnitude speedup compared to existing LP-based approaches.",
            "year": 2020,
            "authors": [
                {
                    "authorId": "46321210",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2109067470",
                    "name": "Huan Zhang"
                },
                {
                    "authorId": "51257183",
                    "name": "Shiqi Wang"
                },
                {
                    "authorId": "2108927851",
                    "name": "Yihan Wang"
                },
                {
                    "authorId": "39400201",
                    "name": "S. Jana"
                },
                {
                    "authorId": "1662772707",
                    "name": "Xue Lin"
                },
                {
                    "authorId": "1793529",
                    "name": "Cho-Jui Hsieh"
                }
            ]
        },
        0.3949886675075028,
        2,
        0.3949886675075028,
        "red"
    ],
    [
        {
            "paperId": "c3ea8eb80bc8ca0b21efa273b9e4a9fd059c65be",
            "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification",
            "abstract": "Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.",
            "year": 2021,
            "authors": [
                {
                    "authorId": "153331364",
                    "name": "Anastasios Nikolas Angelopoulos"
                },
                {
                    "authorId": "153079946",
                    "name": "Stephen Bates"
                }
            ]
        },
        0.31666060796602974,
        1,
        0.6333212159320595,
        "blue"
    ],
    [
        {
            "paperId": "6f9dc6f8519e927d948a13aa7ae0df336f443eb9",
            "title": "Conformalized Quantile Regression",
            "abstract": "Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.",
            "year": 2019,
            "authors": [
                {
                    "authorId": "3295351",
                    "name": "Yaniv Romano"
                },
                {
                    "authorId": "50443159",
                    "name": "Evan Patterson"
                },
                {
                    "authorId": "2006869",
                    "name": "E. Candès"
                }
            ]
        },
        0.31604870601508017,
        1,
        0.6320974120301603,
        "red"
    ],
    [
        {
            "paperId": "33a0803fc10233bd05756ed83db92e8d827d3396",
            "title": "Randomized Smoothing for Stochastic Optimization",
            "abstract": "We analyze convergence rates of stochastic optimization procedures for non-smooth convex optimization problems. By combining randomized smoothing techniques with accelerated gradient methods, we obtain convergence rates of stochastic optimization procedures, both in expectation and with high probability, that have optimal dependence on the variance of the gradient estimates. To the best of our knowledge, these are the first variance-based rates for non-smooth optimization. We give several applications of our results to statistical estimation problems, and provide experimental results that demonstrate the effectiveness of the proposed algorithms. We also describe how a combination of our algorithm with recent work on decentralized optimization yields a distributed stochastic optimization algorithm that is order-optimal.",
            "year": 2011,
            "authors": [
                {
                    "authorId": "1734693",
                    "name": "John C. Duchi"
                },
                {
                    "authorId": "1745169",
                    "name": "P. Bartlett"
                },
                {
                    "authorId": "1721860",
                    "name": "M. Wainwright"
                }
            ]
        },
        0.29502883556316145,
        2,
        0.29502883556316145,
        "blue"
    ],
    [
        {
            "paperId": "43a4a354b67ab6d5531355a368094815d2d2593d",
            "title": "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models",
            "abstract": "Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet.",
            "year": 2018,
            "authors": [
                {
                    "authorId": "2071666",
                    "name": "Sven Gowal"
                },
                {
                    "authorId": "1729912",
                    "name": "Krishnamurthy Dvijotham"
                },
                {
                    "authorId": "49860489",
                    "name": "Robert Stanforth"
                },
                {
                    "authorId": "3407947",
                    "name": "Rudy Bunel"
                },
                {
                    "authorId": "32399256",
                    "name": "Chongli Qin"
                },
                {
                    "authorId": "9960452",
                    "name": "J. Uesato"
                },
                {
                    "authorId": "2299479",
                    "name": "Relja Arandjelović"
                },
                {
                    "authorId": "2554720",
                    "name": "Timothy A. Mann"
                },
                {
                    "authorId": "143967473",
                    "name": "Pushmeet Kohli"
                }
            ]
        },
        0.2854581847325869,
        1,
        0.5709163694651738,
        "red"
    ],
    [
        {
            "paperId": "5812dae376cc07b955244a8e1ce11c3e4b9775ac",
            "title": "Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers",
            "abstract": "Recent works have shown the effectiveness of randomized smoothing as a scalable technique for building neural network-based classifiers that are provably robust to $\\ell_2$-norm adversarial perturbations. In this paper, we employ adversarial training to improve the performance of randomized smoothing. We design an adapted attack for smoothed classifiers, and we show how this attack can be used in an adversarial training setting to boost the provable robustness of smoothed classifiers. We demonstrate through extensive experimentation that our method consistently outperforms all existing provably $\\ell_2$-robust classifiers by a significant margin on ImageNet and CIFAR-10, establishing the state-of-the-art for provable $\\ell_2$-defenses. Moreover, we find that pre-training and semi-supervised learning boost adversarially trained smoothed classifiers even further. Our code and trained models are available at this http URL .",
            "year": 2019,
            "authors": [
                {
                    "authorId": "40575781",
                    "name": "Hadi Salman"
                },
                {
                    "authorId": "35064203",
                    "name": "Greg Yang"
                },
                {
                    "authorId": "2245984",
                    "name": "Jungshian Li"
                },
                {
                    "authorId": "9325940",
                    "name": "Pengchuan Zhang"
                },
                {
                    "authorId": "49723481",
                    "name": "Huan Zhang"
                },
                {
                    "authorId": "2111443",
                    "name": "Ilya P. Razenshteyn"
                },
                {
                    "authorId": "1815542",
                    "name": "Sébastien Bubeck"
                }
            ]
        },
        0.2625114926952598,
        1,
        0.5250229853905196,
        "red"
    ],
    [
        {
            "paperId": "bee044c8e8903fb67523c1f8c105ab4718600cdb",
            "title": "Explaining and Harnessing Adversarial Examples",
            "abstract": "Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",
            "year": 2014,
            "authors": [
                {
                    "authorId": "153440022",
                    "name": "I. Goodfellow"
                },
                {
                    "authorId": "1789737",
                    "name": "Jonathon Shlens"
                },
                {
                    "authorId": "2574060",
                    "name": "Christian Szegedy"
                }
            ]
        },
        0.2509537268553569,
        1,
        0.5019074537107138,
        "gray"
    ],
    [
        {
            "paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
            "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
            "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
            "year": 2017,
            "authors": [
                {
                    "authorId": "143826246",
                    "name": "A. Madry"
                },
                {
                    "authorId": "17775913",
                    "name": "Aleksandar Makelov"
                },
                {
                    "authorId": "152772922",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "2754804",
                    "name": "Dimitris Tsipras"
                },
                {
                    "authorId": "2869958",
                    "name": "Adrian Vladu"
                }
            ]
        },
        0.24350707951144882,
        1,
        0.48701415902289763,
        "red"
    ],
    [
        {
            "paperId": "acd3f1ee1c67bd56a8b6454f413b89a28d34288e",
            "title": "Distribution-Free Prediction Sets",
            "abstract": "This article introduces a new approach to prediction by bringing together two different nonparametric ideas: distribution-free inference and nonparametric smoothing. Specifically, we consider the problem of constructing nonparametric tolerance/prediction sets. We start from the general conformal prediction approach, and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution. The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cutoff values. Under standard smoothness conditions, we get an asymptotic efficiency result that is near optimal for a wide range of function classes. But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size. The performance of our method is investigated through simulation studies and illustrated in a real data example.",
            "year": 2013,
            "authors": [
                {
                    "authorId": "145258822",
                    "name": "Jing Lei"
                },
                {
                    "authorId": "145607066",
                    "name": "J. Robins"
                },
                {
                    "authorId": "1733999",
                    "name": "L. Wasserman"
                }
            ]
        },
        0.229558266488309,
        1,
        0.459116532976618,
        "red"
    ],
    [
        {
            "paperId": "20d7d0c9c3b48d67d11dfe3c3d0aae737ea4bf8d",
            "title": "Tight Verification of Probabilistic Robustness in Bayesian Neural Networks",
            "abstract": "We introduce two algorithms for computing tight guarantees on the probabilistic robustness of Bayesian Neural Networks (BNNs). Computing robustness guarantees for BNNs is a significantly more challenging task than verifying the robustness of standard Neural Networks (NNs) because it requires searching the parameters' space for safe weights. Moreover, tight and complete approaches for the verification of standard NNs, such as those based on Mixed-Integer Linear Programming (MILP), cannot be directly used for the verification of BNNs because of the polynomial terms resulting from the consecutive multiplication of variables encoding the weights. Our algorithms efficiently and effectively search the parameters' space for safe weights by using iterative expansion and the network's gradient and can be used with any verification algorithm of choice for BNNs. In addition to proving that our algorithms compute tighter bounds than the SoA, we also evaluate our algorithms against the SoA on standard benchmarks, such as MNIST and CIFAR10, showing that our algorithms compute bounds up to 40% tighter than the SoA.",
            "year": 2024,
            "authors": [
                {
                    "authorId": "2123357456",
                    "name": "Ben Batten"
                },
                {
                    "authorId": "2057764204",
                    "name": "Mehran Hosseini"
                },
                {
                    "authorId": "1715165",
                    "name": "A. Lomuscio"
                }
            ]
        },
        0.21588242425706136,
        1,
        0.4317648485141227,
        "red"
    ],
    [
        {
            "paperId": "d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad",
            "title": "Intriguing properties of neural networks",
            "abstract": "Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. \nFirst, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. \nSecond, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.",
            "year": 2013,
            "authors": [
                {
                    "authorId": "2574060",
                    "name": "Christian Szegedy"
                },
                {
                    "authorId": "2563432",
                    "name": "Wojciech Zaremba"
                },
                {
                    "authorId": "1701686",
                    "name": "I. Sutskever"
                },
                {
                    "authorId": "143627859",
                    "name": "Joan Bruna"
                },
                {
                    "authorId": "1761978",
                    "name": "D. Erhan"
                },
                {
                    "authorId": "153440022",
                    "name": "I. Goodfellow"
                },
                {
                    "authorId": "2276554",
                    "name": "R. Fergus"
                }
            ]
        },
        0.18640495543694785,
        1,
        0.3728099108738957,
        "blue"
    ],
    [
        {
            "paperId": "2410923ed90b099e3f5565b63e789f10bf70ec4c",
            "title": "Formal Security Analysis of Neural Networks using Symbolic Intervals",
            "abstract": "Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world security-critical domains including autonomous vehicles and collision avoidance systems, formally checking security properties of DNNs, especially under different attacker capabilities, is becoming crucial. Most existing security testing techniques for DNNs try to find adversarial examples without providing any formal security guarantees about the non-existence of such adversarial examples. Recently, several projects have used different types of Satisfiability Modulo Theory (SMT) solvers to formally check security properties of DNNs. However, all of these approaches are limited by the high overhead caused by the solver. \nIn this paper, we present a new direction for formally checking security properties of DNNs without using SMT solvers. Instead, we leverage interval arithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike existing solver-based approaches, is easily parallelizable. We further present symbolic interval analysis along with several other optimizations to minimize overestimations of output bounds. \nWe design, implement, and evaluate our approach as part of ReluVal, a system for formally checking security properties of Relu-based DNNs. Our extensive empirical results show that ReluVal outperforms Reluplex, a state-of-the-art solver-based system, by 200 times on average. On a single 8-core machine without GPUs, within 4 hours, ReluVal is able to verify a security property that Reluplex deemed inconclusive due to timeout after running for more than 5 days. Our experiments demonstrate that symbolic interval analysis is a promising new direction towards rigorously analyzing different security properties of DNNs.",
            "year": 2018,
            "authors": [
                {
                    "authorId": "51257183",
                    "name": "Shiqi Wang"
                },
                {
                    "authorId": "40428350",
                    "name": "Kexin Pei"
                },
                {
                    "authorId": "2042956146",
                    "name": "J. Whitehouse"
                },
                {
                    "authorId": "152211006",
                    "name": "Junfeng Yang"
                },
                {
                    "authorId": "39400201",
                    "name": "S. Jana"
                }
            ]
        },
        0.17330172333284627,
        1,
        0.34660344666569254,
        "red"
    ],
    [
        {
            "paperId": "a4dccdb27a88c6c8c47a0ac98c66171dc8c40514",
            "title": "Bounded and Unbounded Verification of RNN-Based Agents in Non-deterministic Environments",
            "abstract": "Weconsider closed-loopAgent-Environment Systems(AESs),where the agent is controlled by a Recurrent Neural Network (RNN) with ReLU activations in a non-deterministic environment. We introduce a new approach based on Mixed-Integer Linear Programming to verify such systems, which allows for more optimised complete and sound verification of bounded temporal properties of such AESs. Using our approach, we additionally, devise a sound algorithm for the unbounded verification of such AESs for the first time",
            "year": 2023,
            "authors": [
                {
                    "authorId": "2057764204",
                    "name": "Mehran Hosseini"
                },
                {
                    "authorId": "1715165",
                    "name": "A. Lomuscio"
                }
            ]
        },
        0.11906782874675098,
        1,
        0.23813565749350196,
        "gray"
    ],
    [
        {
            "paperId": "3a70562df004e08d91b125e6d15255e31e445efa",
            "title": "PettingZoo: Gym for Multi-Agent Reinforcement Learning",
            "abstract": "This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle (\"AEC\") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning (\"MARL\"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.",
            "year": 2020,
            "authors": [
                {
                    "authorId": "153204999",
                    "name": "J. K. Terry"
                },
                {
                    "authorId": "2056120610",
                    "name": "Benjamin Black"
                },
                {
                    "authorId": "34659542",
                    "name": "Nathaniel Grammel"
                },
                {
                    "authorId": "2051798853",
                    "name": "Mario Jayakumar"
                },
                {
                    "authorId": "1723419097",
                    "name": "Ananth Hari"
                },
                {
                    "authorId": "2165381307",
                    "name": "Ryan Sullivan"
                },
                {
                    "authorId": "2111820310",
                    "name": "L. Santos"
                },
                {
                    "authorId": "2226755530",
                    "name": "Rodrigo Perez"
                },
                {
                    "authorId": "1972440058",
                    "name": "Caroline Horsch"
                },
                {
                    "authorId": "1976106051",
                    "name": "Clemens Dieffendahl"
                },
                {
                    "authorId": "1693750261",
                    "name": "Niall L. Williams"
                },
                {
                    "authorId": "1976100877",
                    "name": "Yashas Lokesh"
                },
                {
                    "authorId": "145451543",
                    "name": "Praveen Ravi"
                }
            ]
        },
        0.06786278389745844,
        1,
        0.13572556779491687,
        "red"
    ],
    [
        {
            "paperId": "2c20e7220269b28fb1935a83d0e7f2db330aa691",
            "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning",
            "abstract": null,
            "year": 2017,
            "authors": [
                {
                    "authorId": "1684175",
                    "name": "B. Biggio"
                },
                {
                    "authorId": "1710171",
                    "name": "F. Roli"
                }
            ]
        },
        0.025426471160342287,
        1,
        0.050852942320684574,
        "blue"
    ],
    [
        {
            "paperId": "b0dc598adda48acab590f95a5985fcc7abf2aca9",
            "title": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
            "abstract": null,
            "year": 2017,
            "authors": [
                {
                    "authorId": "40348902",
                    "name": "Guy Katz"
                },
                {
                    "authorId": "1680661",
                    "name": "Clark W. Barrett"
                },
                {
                    "authorId": "1699040",
                    "name": "D. Dill"
                },
                {
                    "authorId": "145533911",
                    "name": "Kyle D. Julian"
                },
                {
                    "authorId": "2275756",
                    "name": "Mykel J. Kochenderfer"
                }
            ]
        },
        0.025426471160342287,
        1,
        0.050852942320684574,
        "gray"
    ],
    [
        {
            "paperId": "72e55b90b5b791646266b0da8f6528d99aa96be5",
            "title": "An Abstraction-Refinement Approach to Verification of Artificial Neural Networks",
            "abstract": null,
            "year": 2010,
            "authors": [
                {
                    "authorId": "1716255",
                    "name": "Luca Pulina"
                },
                {
                    "authorId": "1834340",
                    "name": "A. Tacchella"
                }
            ]
        },
        0.025426471160342287,
        1,
        0.050852942320684574,
        "gray"
    ],
    [
        {
            "paperId": "1776b5ba5d2b17f6e6a043d57d36126e2af90315",
            "title": "Algorithmic Learning in a Random World",
            "abstract": null,
            "year": 2005,
            "authors": [
                {
                    "authorId": "145675281",
                    "name": "V. Vovk"
                },
                {
                    "authorId": "1793317",
                    "name": "A. Gammerman"
                },
                {
                    "authorId": "145500409",
                    "name": "G. Shafer"
                }
            ]
        },
        0.025426471160342287,
        1,
        0.050852942320684574,
        "blue"
    ],
    [
        {
            "paperId": "4edbaec77e146d86f1ca3947f8bac662e2c8cede",
            "title": "NNV 2.0: The Neural Network Verification Tool",
            "abstract": null,
            "year": 2023,
            "authors": [
                {
                    "authorId": "144194822",
                    "name": "Diego Manzanas Lopez"
                },
                {
                    "authorId": "2149228207",
                    "name": "Sung-Woo Choi"
                },
                {
                    "authorId": "2135390",
                    "name": "Hoang-Dung Tran"
                },
                {
                    "authorId": "2427495",
                    "name": "Taylor T. Johnson"
                }
            ]
        },
        0.025426471160342287,
        1,
        0.050852942320684574,
        "red"
    ]
]